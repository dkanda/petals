<p align="center">
    <img src="https://i.imgur.com/7eR7Pan.png" width="400"><br>
    <h1>Heliopetals</h1>
    Run large language models at home, BitTorrent-style.<br>
    Fine-tuning and inference <a href="https://github.com/bigscience-workshop/petals#benchmarks">up to 10x faster</a> than offloading
    <br><br>
    <a href="https://pypi.org/project/heliopetals/"><img src="https://img.shields.io/pypi/v/heliopetals.svg?color=green"></a>
    <a href="https://discord.gg/tfHfe8B34k"><img src="https://img.shields.io/discord/865254854262652969?label=discord&logo=discord&logoColor=white"></a>
    <br>
</p>

Generate text with distributed **Llama 3.1** (up to 405B), **Mixtral** (8x22B), **Falcon** (40B+) or **BLOOM** (176B) and fine‚Äëtune them for your own tasks &mdash; right from your desktop computer or Google Colab:

```python
from transformers import AutoTokenizer
from heliopetals import AutoDistributedModelForCausalLM

# Choose any model available at https://health.petals.dev
model_name = "meta-llama/Meta-Llama-3.1-405B-Instruct"

# Connect to a distributed network hosting model layers
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoDistributedModelForCausalLM.from_pretrained(model_name)

# Run the model as if it were on your computer
inputs = tokenizer("A cat sat", return_tensors="pt")["input_ids"]
outputs = model.generate(inputs, max_new_tokens=5)
print(tokenizer.decode(outputs[0]))  # A cat sat on a mat...
```

<p align="center">
    üöÄ &nbsp;<b><a href="https://colab.research.google.com/drive/1uCphNY7gfAUkdDrTx21dZZwCOUDCMPw8?usp=sharing">Try now in Colab</a></b>
</p>

## Setting Up Your Own Heliopetals Swarm

For sensitive data or to create your own private network, you can set up your own Heliopetals swarm. This allows you to have a central server that your peers connect to, and it also enables the token tracking feature.

### Step 1: Set Up Backbone Peers

It's recommended to have a few CPU-only machines that are always online to act as bootstrap peers. These peers help new GPU servers join the network.

To start a bootstrap peer, run this command:

```bash
python -m heliopetals.cli.run_dht --host_maddrs /ip4/0.0.0.0/tcp/31337 --identity_path bootstrap1.id
```

You'll see an output with an address. You'll use this as the `--initial_peers` for your servers.

### Step 2: Start Heliopetals Servers

Now, you can start the Heliopetals servers on your GPU machines. Use the `--initial_peers` argument to point to your bootstrap peers:

```bash
export INITIAL_PEERS=/ip4/YOUR_ADDRESS_HERE/tcp/31337/p2p/QmTPAIfThisIsMyAddressGoFindYoursnCfj
python -m heliopetals.cli.run_server bigscience/bloom --initial_peers $INITIAL_PEERS
```

### Step 3: Use the Model

To use the model in your private swarm, add the `initial_peers` argument:

```python
INITIAL_PEERS = [
    "/ip4/YOUR_ADDRESS_HERE/tcp/31337/p2p/QmTPAIfThisIsMyAddressGoFindYoursnCfj",
]

model = AutoDistributedModelForCausalLM.from_pretrained("bigscience/bloom", initial_peers=INITIAL_PEERS)
```

### Token Tracking

When you run your own swarm, the server will automatically track the number of tokens generated by each peer. The token counts are logged to a file named `token_log.txt` in the server's working directory.

## Connect your GPU and increase Heliopetals capacity

Heliopetals is a community-run system &mdash; we rely on people sharing their GPUs. You can help serving one of the [available models](https://health.petals.dev) or host a new model from ü§ó [Model Hub](https://huggingface.co/models)!

As an example, here is how to host a part of [Llama 3.1 (405B) Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct) on your GPU:

ü¶ô **Want to host Llama?** [Request access](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct) to its weights, then run `huggingface-cli login` in the terminal before loading the model.

üêß **Linux + Anaconda.** Run these commands for NVIDIA GPUs (or follow [this](https://github.com/bigscience-workshop/petals/wiki/Running-on-AMD-GPU) for AMD):

```bash
conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia
pip install git+https://github.com/bigscience-workshop/heliopetals
python -m heliopetals.cli.run_server meta-llama/Meta-Llama-3.1-405B-Instruct
```

ü™ü **Windows + WSL.** Follow [this guide](https://github.com/bigscience-workshop/petals/wiki/Run-Heliopetals-server-on-Windows) on our Wiki.

üêã **Docker.** Run our [Docker](https://www.docker.com) image for NVIDIA GPUs (or follow [this](https://github.com/bigscience-workshop/petals/wiki/Running-on-AMD-GPU) for AMD):

```bash
sudo docker run -p 31330:31330 --ipc host --gpus all --volume heliopetals-cache:/cache --rm \
    learningathome/heliopetals:main \
    python -m heliopetals.cli.run_server --port 31330 meta-llama/Meta-Llama-3.1-405B-Instruct
```

üçè **macOS + Apple M1/M2 GPU.** Install [Homebrew](https://brew.sh/), then run these commands:

```bash
brew install python
python3 -m pip install git+https://github.com/bigscience-workshop/heliopetals
python3 -m heliopetals.cli.run_server meta-llama/Meta-Llama-3.1-405B-Instruct
```
